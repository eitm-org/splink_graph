{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_31103/3654128273.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunctions\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwindow\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mWindow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mw\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msql\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mRow\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mpyspark\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "import pyspark.sql.functions as f\n",
    "from pyspark.sql.window import Window as w\n",
    "\n",
    "from pyspark.sql import Row\n",
    "import pyspark\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark = (\n",
    "  SparkSession.builder.master('local[*]')\n",
    "    .appName('synth')\n",
    "    \n",
    "    #these confs are spark installation specific (these specific confs are for MOJ AP)\n",
    "    \n",
    "    .config('spark.driver.memory', '8g')\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\") \n",
    "    \n",
    "    \n",
    "    #these confs are required for splink_graph to run properly \n",
    "\n",
    "    .config(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\")\n",
    "    .config(\"spark.executorEnv.ARROW_PRE_0_15_IPC_FORMAT\", \"1\")\n",
    "    .getOrCreate()\n",
    "  )\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e= spark.read.parquet(\"notebooks/data/df_e.snappy.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_e = df_e.select(\"tf_adjusted_match_prob\",\"unique_id_l\",\"unique_id_r\") # only keeping information needed for graph processing\n",
    "\n",
    "# renaming columns left to more palatable column names\n",
    "df_e = (df_e.withColumnRenamed(\"unique_id_l\",\"src\").\n",
    "        withColumnRenamed(\"unique_id_r\",\"dst\").\n",
    "        withColumnRenamed(\"tf_adjusted_match_prob\",\"weight\"))\n",
    "\n",
    "df_e.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splink_graph.utils import _graphharmoniser\n",
    "\n",
    "help(_graphharmoniser)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#ensure src/dst are always in the same order in order to avoid join problems\n",
    "df_e = _graphharmoniser (df_e,colsrc=\"src\",coldst=\"dst\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#filter so only edges above threshold form a graph\n",
    "df_e = df_e.filter(f.col(\"weight\")>0.95) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splink_graph.cc import nx_connected_components\n",
    "help(nx_connected_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#run connected components algorithm to create dataframe containing cluster_id and node_id\n",
    "\n",
    "ccs = nx_connected_components(spark,df_e,src=\"src\",dst=\"dst\",weight_colname='weight', \n",
    "                       cluster_id_colname='cluster_id', \n",
    "                       cc_threshold=0.95)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this is temporary. Algorithms now need cluster_id as an integer so cluster_id is casted to Integer\n",
    "ccs = ccs.withColumn(\"cluster_id\",f.col(\"cluster_id\").cast(IntegerType()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#show a bit of the cc dataframe\n",
    "ccs.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# there are clusters of 2,3,4,5,6 nodes\n",
    "ccs.groupBy(\"cluster_id\").count().groupBy(\"count\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# have the edge dataframe include the cluster_id\n",
    "edge_df = ccs.join(df_e,on=ccs[\"node_id\"]==df_e[\"src\"]).drop(\"node_id\")\n",
    "edge_df = edge_df.withColumn(\"distance\",f.round(1.01 - f.col(\"weight\"),2))\n",
    "edge_df.show(8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# start with some basic cluster statistics\n",
    "from splink_graph.cluster_metrics import cluster_basic_stats\n",
    "bcs=cluster_basic_stats(edge_df)\n",
    "bcs.sort(f.col(\"density\").asc()).show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splink_graph.cluster_metrics import cluster_main_stats\n",
    "cms=cluster_main_stats(edge_df)\n",
    "cms.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splink_graph.cluster_metrics import cluster_connectivity_stats\n",
    "ccs=cluster_connectivity_stats(edge_df)\n",
    "ccs.show(6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splink_graph.cluster_metrics import number_of_bridges\n",
    "\n",
    "nb = number_of_bridges(edge_df)\n",
    "\n",
    "nb.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb.groupby(\"number_of_bridges\").count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splink_graph.edge_metrics import bridge_edges\n",
    "\n",
    "br_e = bridge_edges(edge_df)\n",
    "br_e.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from splink_graph.node_metrics import eigencentrality\n",
    "\n",
    "eigenc = eigencentrality(edge_df)\n",
    "eigenc.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "kernelspec": {
   "display_name": "data_linkage",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
